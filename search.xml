<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[文献之美-Non-local Neural Networks]]></title>
    <url>%2F2019%2F05%2F11%2F%E6%96%87%E7%8C%AE%E4%B9%8B%E7%BE%8E-Non-local%20Neural%20Networks%2F</url>
    <content type="text"><![CDATA[1.论文概要研究背景：✡︎论文链接。远程依赖性的获取一直是神经网络的设计核心，图像数据有别于序列顺序信息，对远程依赖性一般依靠于更深层次的堆叠卷积运算。一味的堆叠使得网络计算效率底下、优化困难。为了应对这样的局限，本文对计算机视觉中非局部平均运算进行推广，直观的理解Non-local network就是对一个位置学习全局(input feature map)相对于此位置的权重和。由于此处的“位置”可以是时间，像素点和序列信息，所以其在视频、图像和NLP中都可以有效应用。与堆叠卷积渐进学习不同，non-local运算通过计算任意两位置的相关性。此特性可以有效减少网络层数，算法优点：non-local运算可以通过计算任意距离的两个位置点相互作用来捕获全局依赖性。而且不需要大量的块堆叠，即使只有几层就可以达到理想的效果。与此同时，non-local的计算支持可变输入，所以可以轻松的和其他模块结合。Non-local mean是一种经典的筛选算法，用于计算图像中所有像素的加权平均值。实验验证：基于四个常见的高光谱数据集，与10种不同的高光谱图像分类算法对比，在精度和时间复杂度上本文算法都很有竞争力。2.算法应用Non-local$$\mathbf{y}{i}=\frac{1}{\mathcal{C}(\mathbf{x})} \sum{\forall j} f\left(\mathbf{x}{i}, \mathbf{x}{j}\right) g\left(\mathbf{x}_{j}\right)$$下面三个公式分别可以应用到上式的f函数。$$f\left(\mathbf{x}{i}, \mathbf{x}{j}\right)=e^{\mathbf{x}{i}^{T} \mathbf{x}{j}}$$$$f\left(\mathbf{x}{i}, \mathbf{x}{j}\right)=e^{\theta\left(\mathbf{x}{i}\right)^{T} \phi\left(\mathbf{x}{j}\right)}$$$$f\left(\mathbf{x}{i}, \mathbf{x}{j}\right)=\theta\left(\mathbf{x}{i}\right)^{T} \phi\left(\mathbf{x}{j}\right)$$Non-local块的最终表示如下式：$$\mathbf{z}{i}=W{z} \mathbf{y}{i}+\mathbf{x}{i}$$non-local的块结构：残差单元的改进如图1，从左到右网络设计图2给出了3.实验对比实验☛参考文献[1] D. Han, J. Kim, and J. Kim, “Deep pyramidal residual networks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017,pp. 6307–6315, doi: 10.1109/CVPR.2017.668]]></content>
      <tags>
        <tag>Non-local</tag>
        <tag>图像处理</tag>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文献之美-用于场景分割的双重注意网]]></title>
    <url>%2F2019%2F04%2F15%2F%E6%96%87%E7%8C%AE%E4%B9%8B%E7%BE%8E-%E7%94%A8%E4%BA%8E%E5%9C%BA%E6%99%AF%E5%88%86%E5%89%B2%E7%9A%84%E5%8F%8C%E9%87%8D%E6%B3%A8%E6%84%8F%E7%BD%91%2F</url>
    <content type="text"><![CDATA[1.论文概要✡︎论文链接。本文通过自注意力机制来获取更多上下文依赖关系来解决场景分割任务。提出了双重注意网络（DANet）来自适应地集成本地特征及其全局依赖性。在传统的FCN上附加两种类型的注意模块，分别对空间尺寸和通道建模。其中空间位置注意力模块通过自注意力机制来获取任意两个位置之间的空间依赖性，波段注意力模块中通过类似的自注意力机制来捕获任意两个波段间的依赖关系。最后，融合两个注意模块的输出以进一步增强特征表示。网络架构图如下：]]></content>
      <tags>
        <tag>自注意力机制</tag>
        <tag>场景分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文献之美-残差注意力网对图像的分类]]></title>
    <url>%2F2019%2F04%2F02%2F%E6%96%87%E7%8C%AE%E4%B9%8B%E7%BE%8E-%E6%AE%8B%E5%B7%AE%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E5%AF%B9%E5%9B%BE%E5%83%8F%E7%9A%84%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[1.论文概要研究背景：✡︎论文链接。本文提出了“残差注意力网络”，一种使用注意机制的卷积神经网络，以端到端的训练方式与前沿的前馈网络架构相结合。本文算法有两个优势，首先可以广泛捕获不同类型注意力特征图，其次残差块的加入可以使网络可以轻松拓展到非常深的层次。2.算法应用网络框架设计==未完待续==残差块由两部分构成，trunk branch和mask branch。主干分支主要用于特征处理并能应用于任何前沿的网络架构。网络参数设置☛参考文献[1] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In NIPS, 2015]]></content>
      <tags>
        <tag>图像分类</tag>
        <tag>残差网</tag>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文献之美-波段加权网对高光谱图像的分类]]></title>
    <url>%2F2019%2F03%2F28%2F%E6%96%87%E7%8C%AE%E4%B9%8B%E7%BE%8E-%E6%B3%A2%E6%AE%B5%E5%8A%A0%E6%9D%83%E7%BD%91%E5%AF%B9%E9%AB%98%E5%85%89%E8%B0%B1%E5%9B%BE%E5%83%8F%E7%9A%84%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[1.论文概要研究背景：✡︎论文链接。高光谱遥感图像使用数百个波段来描述地面区域的精细光谱信息。这其中不可避免地包含大量冗余以及噪声频带。发现最优波段并对波段之间的关系建模是处理数据和提高后续分类任务性能的有效手段。算法改进：本文通过设计注意力模块并和传统卷积神经网络融合用于高光谱图像分类，提出了波段加权网络。网络模拟波段间关系建模，根据重要性进行加权。一个突出特征是它可以为不同的样本分配不同的权重。对于光谱高维问题常见的两种解决方法分别为特征提取(采用相关算法计算出一组有用的特征，缺点易破坏数据的原始物理结构且计算成本高)、波段选择(选择有利信息量大的波段)。本文波段加权网将注意力模块集成到CNN网络，通过注意力机制从数据中学习不同波段在高光谱图像中的相对重要性，能有效的建模起波段之间的关系。2.算法应用高光谱的波段加权设$p_i是高光谱图像H的一个像素，波段加权中会找一个N\times1的向量v_i(N表示高光谱图像的波段数$其中$v_i$的第$j$个元素代表波段$j$在样本$p_i)$中对分类的权重大小，加权后的样本$q_i=p_i*v_i(点积)$网络基本单元本文中，仅使用光谱信息进行分类，因此1-D CNN为主要网络结构。在注意力模块设计中，会为每个波段生成一个权重，然后对加权后的特征图进行分类。1-D CNN主要由1-D卷积，1-D池化和全连接层组成。由于使用1-D操作，因此特征映射只是一个向量。用全连接的方式设计波段加权模块。采用形为瓶颈式的网络结构来减少参数数量如图1.$N$表示波段数，$B$表示隐层神经元数量。波段加权过程用$$$q_i=\sigma_2(T_2\sigma_1(T_1p_i))p_i$$$表示,其中$T_1$是权重矩阵为$B\timesN$的第一层，$T_2$是权重矩阵为$N\timesB$的第二层，$\sigma_1$用sigmod激活函数，$\sigma_2$用softmax激活函数。文中还指出用全连接层而不用卷积层是出于在对波段关系建模方面，相关的波段不一定存在于某些局部连续频谱空间中。分类网络采用文献[1]中的网络，不同的是卷积核尺寸设计为$1\times3$3.实验数据集采用IP和UP。batch size设置为64，epoch设置为1000，网络各参数设置如下图2.与其他1-D光谱分类方法进行对比实验。并且通过消融学习(Ablation study)来验证有效性。训练集中IP取20%、UP取10%。同时取10%验证集，在训练的过程中，监测验证损失以提前停止训练。在消融对比中，原方法与去掉加权模块后的对比，显示了注意力模块的有效性。☛参考文献[1] Y. Chen, H. Jiang, C. Li, X. Jia, and P. Ghamisi, “Deep feature extraction and classification of hyperspectral images based on convolutional neural networks,” IEEE Transactions on Geoscience and Remote Sensing, vol. 54, no. 10, pp. 6232–6251, 2016.]]></content>
      <tags>
        <tag>图像分类</tag>
        <tag>高光谱图像</tag>
        <tag>遥感</tag>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文献之美-混合深度残差网用于高光谱图像分类]]></title>
    <url>%2F2019%2F03%2F19%2F%E6%96%87%E7%8C%AE%E4%B9%8B%E7%BE%8E-%E6%B7%B7%E5%90%88%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%94%A8%E4%BA%8E%E9%AB%98%E5%85%89%E8%B0%B1%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[1.论文概要研究背景：本文是17年的会议论文，网络设计创新点和实验内容不多，所以简单的总结一下。✡︎论文链接。随着网络层数的加深，网络学习时不同层级间存在相关性。所提出的DRN可以有效的克服由于网络深度增加和有限的训练样本引起的分类准确性的降低。利用混合机制合并网络中不同分层特征。实验验证：在 Indian Pine数据集随机取10%训练样本训练学习。2.算法应用网络架构创新文中提出的Hybrid deep residual network（DRN）特点在于，层数深，另一点文中提出不同层的输出可以表示不同尺度的特征，它们可以提供互补相关的信息。因此，引入混合机制以充分利用多尺度特征。多尺度特征由最后对应池化层处理后逐点相加融合。本论文网络由九个残差块构成，每个块中有两个卷积层。整个体系结构可以根据卷积核的数量分为三个阶段，分别为16，32和64个卷积核。在普通残差网设计的基础上，对跳跃连接的设定进行改进，一个残差块中跳跃连接不仅仅存在其相应的残差块中，而是存在于整个网络中，对不同层级间的特征进行融合，如下图。3.实验实验结果在17年的时间点上看，分类精度较高。因为在Indian Pine数据集只取10%的训练样本。OA为98.44，AA为98.23.时间原文没有提，由于是深层网络，时间预计很大。]]></content>
      <tags>
        <tag>图像分类</tag>
        <tag>高光谱图像</tag>
        <tag>残差网</tag>
        <tag>HYBRID</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文献之美-基于超像素的3D深层神经网络对高光谱图像分类]]></title>
    <url>%2F2019%2F03%2F13%2F%E6%96%87%E7%8C%AE%E4%B9%8B%E7%BE%8E-%E5%9F%BA%E4%BA%8E%E8%B6%85%E5%83%8F%E7%B4%A0%E7%9A%843D%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AF%B9%E9%AB%98%E5%85%89%E8%B0%B1%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[1.论文概要研究背景：✡︎论文链接。高光谱图像（HSI）由数百个波段组成，其特征在于光谱分辨率高和空间信息丰富。但光谱特征的高维度和有限的训练样本一直影响着HSI分类工作。每个超像素由一组空间相邻、光谱相似的像素组成，具有自适应尺寸形状的特点。用于表示高光谱图像结构时，通过超像素的空间特征图像增加了光谱-空间相似性和多样性。本文把3D DNN与超像素结合，具有更加有效利用光谱空间立方体信息的优势。提供了更平滑的分类图并获得更精确的结果。常规的像素级分类方法易受混合像素和噪声的影响。算法改进：首先，通过对每个超像素内的光谱像素进行线性加权来创建加权特征图像（WFI），以增加对象内的区域一致性。WFI中每个超像素内的光谱像素趋于相似，标记相同。其次，提出了一种基于超像素的三维样本填充方法，分别从HSI和WFI图像构建三维样本，避免不同对象之间的邻域窗口跨越边界。第三，根据填充的3D样本，采用3D CNN从HSI中提取3D谱空间特征。同时，构建3D递归卷积神经网络（3D RCNN）进一步利用WFI的空间连续性信息，抑制分类结果中的噪声。实验验证：数据集采用Pavia University、Groundtruth of Pavia University和Indian Pines。在所有实验的10次独立重复。对于帕维亚大学和印第安松树图像，1％的标记样本被随机选择并用于训练，其余用于测试。对于帕维亚中心图像，0.1％的标记样本被随机选择并用于训练。2.算法应用超像素分割经过超像素分割后的图像形成一个自适应互不重叠且同质的区域集合。本文通过超像素构造WFI图像，促进分类图中谱空特征一致性。通过组合每个超像素内的线性加权光谱像素构造WFI。平均特征图像mean feature image (MFI)通过简单地平均每个超像素内的光谱像素并将平均值分配给同一超像素内的所有光谱像素来构造MFI。如图1(a)原始图像被分割成两个超像素，表示为{SP1，SP2}。 SP1中的光谱像素彼此相似，并且SP2中存在几个明显的混合光谱像素。红色圆圈表示一些混合光谱像素。图1(b)(c)分别表示MFI与WFI。与WFI相比，MFI更平滑，但在MFI图像中每个超像素内的光谱像素的多样性较低。WFI在没有混合光谱像素的超像素中具有良好的性能，然而，对于包含混合光谱像素的超像素，它不能消除混合像素的影响并提供充分的区域一致性。但WFI光谱像素相似性与多样性兼顾的特点可以改善由于训练样本较少而陷入过度拟合的问题。所以我们提出3D RCNNs来从WFI中提取3D特征避免混合光谱像素造成的错误分类。因为3D RCNN可以抑制混合光谱像素的影响，同时保持WFI的多样性和一致性。此外，文中提出了一种3D样本填充方法，通过超像素来抑制3D CNN中选择相邻立方窗口时对边界分类的影响。使用超像素内的光谱空间信息来填充3D邻域立方体窗口中超像素边界之外的像素。填充的3D样本保留与中心像素类似的光谱空间信息，从而避免边界的错误分类。由于WFI具有很好的空间连续性，但缺乏结构信息。 HSI可以提供丰富的结构信息，但它总是会导致噪声分类结果。因此，HSI和WFI都用于3D样本构建，以更好地平衡同质区域和结构区域。总结起来，总共有四个步骤创建超像素和构建WFI，构建3D超像素样本，分别通过3D CNN和3D RCNN从HSI和WFI提取3D谱空间特征，以及多特征学习和分类。图2展示了各步骤的关系。WFI利用每个超像素内的局部空间信息改善分类特征图中的空间连续性。对于每个像素$Z_i|_{B\times1}$(B表示光谱波段维)都会与其超像素内的像素$Z_j|_{B\times1}$进行线性加权运算。如下式(1).$\hat Z_i|{B\times1}=\sum{j=1}^J\alpha_{i,j}\times Z_j|{B\times1} \tag{1}$其中$\alpha{i,j}$代表$Z_i|_{B\times1}$和$Z_j|_{B\times1}$两像素的相似程度，表示如下式(2),$h$是预设好的标量。$\hat Z_i|_{B\times1}$最终替代$Z_i|{B\times1}$。 $\alpha{i,j}= \frac{exp(-||Z_i|_{B\times1}-Z_j|_{B\times1}||_2^2/h)}{f_N} \tag{2}$3D超像素样本构建为了充分利用HSI的结构特征和WFI的空间一致性特征，分别在HSI和WFI上构建三维样本。如果相邻窗口内的所有像素属于相同的超像素，因为它们共享相似的结构和光谱信息所以这些像素直接构成3D立方体样本，否则，如果相邻窗口内的中心像素和其他像素位于不同的超像素，则这些像素将由3D样本填充方法填充，以保持结构和光谱信息的相似性。最终位于几何中心的像素划分比边界像素更有可靠性。因此对于超像素每个区域用相应的最大内切矩形进行填充。然后分别在HSI和WFI上映射基本填充图，以获得3D基本填充图像。获得最大内切矩形的过程及填充方法如图3，首先把基本填充图像（base fill image）和领域窗口（Neighborhood window）中心重合，当Base fill map尺寸大于Neighborhood window时，保留位于与中心像素相同的超像素的像素，并将剩余像素替换为3D基本填充图像（base fill image）中相应的像素。当Base fill map尺寸小于Neighborhood window时，扩充基本填充图像（base fill image）个数直到全覆盖领域窗口（Neighborhood window）。然后根据第一种情况中使用的方法填充剩余的光谱像素。值得注意的是，HSI和WFI上的特征学习过程彼此独立。3DCNN提取HSI中的空谱特征，3D RCNN作用于WFI来解决混合光谱像素的问题。3D RCNN具有与3D CNN类似的结构。不同之处在于3D RCNN的每个卷积层的输入由3D平均样本和若干3D特征映射组成。具体计算公式可参考原论文。3.实验实验中，首先确定超像素的数量对分类效果的影响。结果显示数量太小时，难以分割不同的类，太多时影响区域一致性。，所以会出现起初效果改善明显，超过一个固定值时分类效果反而变差。Pavia University最优化时为150个超像素，Indian为100个超像素、Pavia为1000个超像素，因其纹理特征较复杂。考录到计算成本和分类效果的权衡，3D samoles的尺寸大小$L\times L\times B中的L取值11$实验还对比了3D CNN、以及只借助WFI时、WFI和3D样本填充都加入时、及再加入3D RCNN下的三种情况。与其他算法对比显示，本算法在训练样本很少的情况下表现较好，但由于超像素和3D样本填充的加入使得算法时间复杂度增加。使得算法的时间复杂度较大。☛参考文献[1] Shi C , Pun C M . Superpixel-based 3D Deep Neural Networks for Hyperspectral Image Classification[J]. Pattern Recognition, 2017:S0031320317303515.]]></content>
      <tags>
        <tag>超像素</tag>
        <tag>深层神经网络</tag>
        <tag>图像分类</tag>
        <tag>高光谱图像</tag>
        <tag>遥感</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文献之美-用于空谱高光谱图像分类的Pyramidal残差网]]></title>
    <url>%2F2019%2F02%2F25%2F%E6%96%87%E7%8C%AE%E4%B9%8B%E7%BE%8E-%E7%94%A8%E4%BA%8E%E7%A9%BA%E8%B0%B1%E9%AB%98%E5%85%89%E8%B0%B1%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%9A%84Pyramidal%E6%AE%8B%E5%B7%AE%E7%BD%91%20-%20%E5%89%AF%E6%9C%AC%20-%20%E5%89%AF%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[1.论文概要研究背景：✡︎论文链接。由于高光谱图像（HSI）本生的高维度和“噪声”导致常规的CNN不能较好的利用高光谱图像的空谱特征，加之随着CNN层数的增加效果反而更差。为了解决以上困扰本文提出深度金字塔（Pyramidal）网用于高光谱图像分类。有别于一般的残差网，Pyramidal残差网类似于金字塔形状逐渐增加卷积层特征图的维度，在网络更深层次涉及更多的特征，同时平衡每层的时间复杂度.同时，应对HSI高复杂性的创新研究线之一是更好的挖掘空间-光谱特征的结合，它的实现比单纯的像素分类效果更好，此类方法通过将每个像素的光谱与其所属的相应结构的尺寸和形状相结合来使分类不确定性降低。算法改进：常规残差网络（ResNet）通过残差块来改进CNN，促进更深层网络的学习并使网络层次更深入。本文通过基于金字塔瓶颈残差块建立网络，使用光谱信息和空间信息来实现快速准确的高光谱分类。与金字塔形似，越接近输出的网络层特征学习图越大。便于学习更多的有价值特征。同时网络对训练样本数量也要求较少。实验验证：基于四个常见的高光谱数据集，与10种不同的高光谱图像分类算法对比，在精度和时间复杂度上本文算法都很有竞争力。2.算法应用网络架构基础采用2D-CNN,但是所有波段都将用于输入数据，为了保留原始光谱特征不采用类似PCA的降维方法。CNN层中包括卷积层、Batch normalization层(减少协方差偏移，在每批特征图上强加高斯分布)、Nonlinearity层(学习非线性表示)和池化层。残差单元的改进如图1，从左到右分别给出了传统残差块、瓶颈残差块和金字塔瓶颈残差块。传统残差块从上到下具有相同的拓扑，瓶颈残差块中的特征图先被缩小然后恢复输入时的大小，这样的结构有助于残差单元更快的执行。最后金字塔残差块单元中，CONV层的通道数逐渐增加，导致各层逐渐变宽。随之带来一个问题，残差块中最终的原始输入特征映射$p_j$(通过跳跃连接而来)与残差函数$F(p_j,w_j)$的结果之间的维度不同，本文采用 [1] 中零填充快捷方式，即添加额外的零条目直到达到增加的维度。网络设计图2给出了模型体系结构，右下角标记出了各个颜色代表的不同功能层。其中$C$为输入模块，$P_1-P_3$被命名为金字塔残差模块，里面包含若干金字塔瓶颈残差单元$B$。Patch单位的数据依次通过如图的五个模块，组成了很深的网络。图中每个$B^{(i)}_j$由几类层堆叠而成。三个Conv层前面分别有Bach normalization层在单元的末端加入Relu激活层。实现了每个Conv层考虑了其输入特征中所有的光谱信息，同时处理了窗口内的空间信息。在$P_2$和$P_3$中加入了由平均池化层构成的下采样层，来减少数据方差并从空间邻域中提取低级别的特征来提供给下一层。在网络设计中采用 [1] 中的方法线性地增加每个残差单元的特征图深度。具体每层滤波器属性取值在原文表1列出。学习率的设置，在前150epochs中设置为0.1，后50epochs设为0.01。3.实验对比实验包含10中不同的分类方法，分别为：1) SVM with radial basis function kernel [2]; 2) RF; 3) MLP;4) extreme learning machine (ELM) [3]; 5) kernel ELM(KELM) [4]; 6) 1-D CNN; 7) 2-D CNN; 8) 3-D-CNN;9) spectral–spatial ResNet (SSRN) [5]; 10) deep fast CNN (DFCNN)实验内容包括四个不同的实验，用于验证相对于标准分类器(SVM,RF,MLP,2D-CNN,3D-CNN)的性能（实验1），不同的训练数据百分比比较（实验2），与最近的两个基于CNN的空谱分类器进行比较（实验3和4）。其中实验二特别在IP和UP数据集，遵循相同配置，尺寸大小为N×7×7，分别使用5％，10％，15％，20％和25％的训练比比较。实验三与SSRN的比较中采用四种不同的空间大小5×5,7×7,9×9,11×11，在IP和KSC数据集中用20%作为训练数据，UP数据集中用10%作为训练数据。实验四与DFCNN比较时采用9×9,15×15,19×19三种空间尺寸大小。实验1结果显示本文算法较其他标准分类器不仅在精度上有所提升，而且用时较短。实验2结果显示在IP数据集中当训练样本达到20%各算法分类精度提升变缓慢，而在UP数据集中同样的效果只需5%。同时在相同训练样本比下，本文算法在分类精度上表现最好。在与同样是根据空谱信息结合进行分类的算法对比中，本文算法在相同输入空间尺寸下，不仅精度有所提高，而且标准差较其他两个方法小。☛参考文献[1] D. Han, J. Kim, and J. Kim, “Deep pyramidal residual networks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017,pp. 6307–6315, doi: 10.1109/CVPR.2017.668[2] B. Waske, S. van der Linden, J. Benediktsson, A. Rabe, and P. Hostert,“Sensitivity of support vector machines to random feature selection in classification of hyperspectral data,” IEEE Trans. Geosci. Remote Sens.,vol. 48, no. 7, pp. 2880–2889, Jul. 2010.[3] G.-B. Huang, H. Zhou, X. Ding, and R. Zhang, “Extreme learning machine for regression and multiclass classification,”IEEE Trans.Syst.,Man, Cybern. B, Cybern., vol. 42, no. 2, pp. 513–529, Apr. 2012.[4] G.-B. Huang and C.-K. Siew, “Extreme learning machine with randomly assigned RBF kernels,” Int. J. Inf.Technol., vol. 11, no. 1, pp. 16–24,2005.[5] Z. Zhong, J. Li, Z. Luo, and M. Chapman, “Spectral–spatial residual network for hyperspectral image classification: A 3-D deep learning framework,” IEEE Trans. Geosci. Remote Sens., vol. 56, no. 2,pp. 847–858, Feb. 2018.]]></content>
      <tags>
        <tag>图像分类</tag>
        <tag>高光谱图像</tag>
        <tag>残差网</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海天一色，现实有别]]></title>
    <url>%2F2019%2F02%2F25%2F%E6%B5%B7%E5%A4%A9%E4%B8%80%E8%89%B2%EF%BC%8C%E7%8E%B0%E5%AE%9E%E6%9C%89%E5%88%AB%2F</url>
    <content type="text"><![CDATA[我从海边来 身上长满了盐清晨 带着希望的太阳如期而至曙光让我发出刺眼的反光偶尔 虚无的快乐后让我注目大海像沙砾中的玻璃渣子 坐在海边捂着耳朵 大海的声音不再进来顿时发现没了声音的大海与天空无异既然万物终究会和云一样转瞬即逝那我为什么不是从天边来那样 我和曙光不再隔空遥望我的底色也不再反光]]></content>
      <tags>
        <tag>杂记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文献之美-残差网图像分割]]></title>
    <url>%2F2019%2F02%2F06%2F%E6%96%87%E7%8C%AE%E4%B9%8B%E7%BE%8E-%E6%AE%8B%E5%B7%AE%E7%BD%91%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%2F</url>
    <content type="text"><![CDATA[1.论文概要研究背景：论文链接。语义图像分割（Semantic image segmentation）一直是CV领域的热点。经过语义分割后，图像中的每个像素被分配一个类别标签。目前语义图像分割常用框架通常为把全连接替换为卷积层的全卷积网络。残差块的引入通过跳跃连接的引入有效的解决了梯度消失的问题。本文通过改进残差网络设计用于语义图像分割。对于图像语义分割，“上下文信息”获取的越多越好。为了获取更多“上下文信息”现有的方法大多存在网络架构过于复杂，大规模的超参数调整的问题。本文提出的金字塔残差块简单而有效。不仅限于文中的任务，是一种可应用于基于ResNet的其他体系结构或任务的通用方法。算法改进：本文提出名为金字塔残差快（pyramid residual block），可以更好的利用上下文信息并增强关键特征。相比传统残差块，Pyramid残差块添加了两部分，首先是聚合基于不同区域上下文信息的Pyramid池化块，然后添加了注意力机制，能通过逐元素乘法运算自适应地重新校准特征响应，从而增强有用特征并抑制次要特征。实验验证：本文提出的金字塔残差块在PASCAL VOC 2012分段数据集中表现出色，并且在标准残差块上大大提高了分割精度。2.算法应用创新点1：Pyramid Pooling Module在图像语义分割中，已经有许多方法更有效地收集背景信息。[1]最先提出金字塔Pooling模块。[1]中金字塔汇集模块附在ResNet的顶部，而本文的金字塔汇集模块嵌入到ResNet的残差块中如图1。具体而言，细粒度或局部信息有助于实现良好的像素级精度，图像的全局背景能够澄清局部模糊性。在深度神经网络中，虽然理论上来自更高层次的特征已经具有超出输入图像尺寸的非常大的感受野，但实际上感受野的尺寸远小于理论尺寸。这阻碍了充分合并重要的上下文信息，Pyramid Pooling Module有效的解决了这个问题。如图[1]所示，改进后的模块将来自残差块主分支的特征映射作为输入。它具有三个并行分支，每个分支包含一个池化层，一个ReLU激活函数，两个卷积层（1×1卷积），一个sigmoid函数和一个上采样操作。三个分支中的池的bin大小分别设置为1×1,2×2和4×4且都为平均池化。池化操作后的第一个卷积层用于降维。之后的第二卷积层用于维度增加，恢复原始信道数。再经过上采样操作后，来自三个并行分支的特征映射通过逐元素求和融合，得到注意力概率分布图（attention probability maps）。创新点2：注意力机制在金字塔池模块的每个分支中，对于第二个卷积后的激活函数，选择sigmoid函数而不是ReLU函数来规范化特征映射的每个元素。最终拿输出的注意力概率分布图和来自残差块主分支的特征图执行逐元素乘法运算。通过这种简单的方式，可以为更有用的特征响应分配更大的权重，从而有助于提高特征可辨性。3.实验实验数据集为PASCAL VOC 2012，广泛用于的语义分割数据集。包含20个对象类别和一个背景类。我们使用[2]提供的额外注释来增加数据，分别生成10582,1449和1456个用于训练，验证和测试的图像。使用随机梯度下降（SGD）训练模型，批量大小为10，最大迭代次数为20K。为了增强数据，对所有训练图像使用随机镜像和随机裁剪。使用已经在ImageNet数据集上预训练的ResNet-50或ResNet-101网络作为基础模型。最后一个残余块的输出由1×1卷积层和softmax非线性处理，以产生最终的像素分割结果。为了探索降维对性能的影响，用金字塔残差块替换ResNet-50的最后残差块（res5c），并将维数降低层的通道数（即图1中的’Conv1’）设置为分别为1024,512,256和128。结果显示合理压缩特征的维数可以在一定程度上提高分割精度;这可能是因为删除了冗余特征。但是，当维数降低太多时，由于判别特征的丢失，结果又会变差。本实验显示当通道数为256时效果最好。为了评估单个组件的有效性，分别保留金字塔池模块和注意力机制两个组件中的一个并移除另一个。实验结果显示每个组件都有利于分割效果，当二者合并时效果最好。由于具有一个金字塔残差块（res5c）的ResNet-50在很大程度上优于基本模型，实验进一步用金字塔残差块替换原始ResNet-50的两个块（res5c和res4f）和三个块（res5c，res4f和res4c）。实验结果显示，具有两个金字塔残差块的模型比仅有一个金字塔残差块的模型带来了0.91％的改进，具有三个金字塔残余块的模型进一步将性能提高了0.71％。因此，本文提出的金字塔残差块对于改进基于ResNet的语义分割系统是简单而有效的。为了探究更深层次的神经网络是否有利于像素方式的语义分割。实验采用预先培训的ResNet-101，其修改与ResNet-50相同。以相同的方式，一个块（res5c），两个块（res5c和res4b22）和三个块（res5c，res4b22和res4b19）分别被金字塔残余块替换。实验结果显示，相同的设置下，ResNet-101在很大程度上优于ResNet-50。此外，当更换的块数增加时，性能从71.27％提高到75.38％。更深层次的ResNet也受益于更多金字塔剩余块。对比实验结果如图2。☛参考文献[1] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,” arXiv preprint arXiv:1612.01105, 2016[2] B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik, “Semantic ´contours from inverse detectors,” in Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, 2011, pp. 991–998.]]></content>
      <tags>
        <tag>图像分类</tag>
        <tag>残差网</tag>
        <tag>图像语义分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git学习笔记]]></title>
    <url>%2F2015%2F03%2F29%2FGit%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[引言Git号称是世界上最先进的分布式版本控制系统，没错玩熟了它你就离高端大气上档次又近了一步。前段时间看实习招聘信息，有些公司都不用去当地办公，只要会Git可以远程完成任务，当时就觉得Git一定要好好学一学，虽然之前弄博客看了一些Git的内容，但当时是单纯的跟着教程去敲的，很多地方都没真正掌握。好记性不如烂键盘，这篇博客是对Git用法的精简，涉及到的都是硬核，便于自己忘了回头查阅，也希望可以帮到其他的初学者。详细的Git手册可以参考Git-Book，这个网站各种语言都有。Git初识与安装要它干嘛：简单地说Git的诞生与一众事物一样，为了进一步解放你的双手。Git让你和队友配合更方便，提供给你们一个协作编辑的平台，并且把你们每次的动作（也可认为是新的版本）信息记录，便于日后分析和版本回退。怎么获取：Git对Linux、Mac和Windows都支持，具体步骤就不详述了，因为本人人穷没用过Mac，而且又境界不高，Linux用的不太六。网上安装教程还是很详细的，Windows用户直接去Git官网下载安装。Git官网下载页面。Git里的行话分布式：学习任何东西，都得先把这个邻域频繁出场且刺眼的“语料”理解透彻，这决定了你是否可以愉快无痛的把玩它。说到Git就想到了分布式版本控制系统，一分为二，版本控制系统前文已经解释了，那分布式好像字面就能理解，但与版本控制系统连起来读意思又不明确了。与集中式对比理解，集中式是一台中央服务器存了一切版本信息，你和队友电脑里啥也没，因为你们没权限，想用要么局域网要么互联网，与主机建立连接后才能修改，改完后主机又把相关信息存起来了。而分布式就不同了，此时你和队友的电脑都是“中央服务器”，因为你们电脑里都存储着全部的版本信息。你们各自的修改信息可以两两互换，也可以几个人借助一个服务器愉快的交流。再也不怕一台机子熄火全员收工了呢。仓库：仓库就是一个目录，目录里的文件动态被Git记录跟踪。一个仓库需要创建和初始化。1234$ mkdir git_breeze //创建目录$ cd git_breeze //打开创建好的目录$ git init //把目录变为Git可以控制的库Initialized empty Git repository in L:/git_test/git_breeze/.git/仓库搞定了，自然要学会往里面放东西、取东西。放东西时可以一次性放一批，逐个文件通过git add 文件名添加。最后通过一条命令git commit -m &quot;写一句概括本次修改的话&quot; 完成入库。暂存区在了解暂存区之前，先要知道Git里的工作区。工作区就是你存放在本地硬盘里的Git文件夹信息。当你把工作区的文件修改后，要传到版本库里。在上文仓库中提到了入库两步骤，首先git add此时可以理解为把文件从工作区传到了暂存区。而git commit是把文件从暂存区传到版本库里的分枝master中。分枝暂存区中说到，master用来接收暂存区的版本信息。master就是一条分支，这条支线上包含完整的版本历史信息，每个版本就是一个节点。HEAD带表当前版本指向master。在多人协作中为了在不影响他人下又能自己灵活支配Git库存储，一般要自己创建一条像master的分支。待自己的任务完成后再与master分支合并。有时当多个人对同一内容分别更改并同时提交时会有冲突，用$ git status查看冲突位置并做更改。123456789$ git checkout -b newbra //创建并切换到新分支newbra$ git checkout master //切换回master分支$ git merge newbra //把newbra分支下的动作合并到master分支//上句默认快速合并，为了分支合并信息受保护，常加参数 --no--ff如下$ git merge --no--ff -m &quot;此时创建新commit，此处写更改信息&quot; newbra$ git branch -d newbra //如需删除刚刚创建的newbra分支执行之$ git branch //实时查看库里的分枝信息标签简单的理解就是给一个版本起一个容易念容易写的名字，以后调的话方便一点。12345$ git tag breeze1.0 //在当前分支最新提交的commit打一个标签$ git tag -a 版本名 -m “描述” 版本commit的id号 //创建有说明的标签$ git tag breeze1.0 commit的id号 //对一个指定commit打标签$ git log --pretty=oneline --abbrev-commit //查看历史commit id号$ git tag //查看所有标签Git里的雷区Github与GitGit、Github一个比一个出名，出名到一部分未入深坑的用户都不知道谁是谁什么关系。Git是分布式版本控制系统。顾名思义一个仓库可以分布到不同主机。但是宗旨是多人协作下多版本管理，为了协作方便自然要有一个大家随时都能存取查改的平台，Github应用而生，为用户提供Git仓库托管，让本地与远程同步。二者的传输通过SSH加密传输。所以要获取SSH Key才能愉快玩耍，获取方法如下。1$ ssh-keygen -t rsa -C &quot;你的邮箱&quot;获取后用户目录下会有.ssh文件夹放了两个密匙一个私匙和一个公匙，私匙要保管好。最后到你Github设置里把密匙输入就OK了。推送本地仓库到远程Github非常简单。首先你得有一个向前文那样已经$ Git init初始化好的本地仓库。接着在Github里创建一个同名repository。接着网页就提示你怎么做，你只需在本地仓库下的Git Bash里输入网页提示的第二种方法如下。到此你的本地仓库就上传到了你的Github账户里了12$ git remote add origin https://github.com/你的Github账户名/你的仓库名$ git push -u origin master接着逆向思维一下，上面是本地Git上传到远程Github，那么怎么把远程Github克隆到本地Git库里呢。一句搞定。1$ git clone https://github.com/你的Github账户名/你的仓库名Git里的套路版本信息查改：12345$ git status //实时查看仓库状态$ git diff 文档名 //查看文档前后改动的内容$ git diff HEAD -- 文档名 //查看工作区和版本库里最新版本的区别$ git log //查看历史提交日志$ git reflog //查看你敲过的历史命令以上是常用的查看版本信息的命令，Git的过人之处并不是记录版本信息，而是帮你一次次在各版本间跳跃转换，你可以跳跃回上个版本或任意版本，或是后悔了在撤销此次转换再恢复过来，get reset就是干这个活的，具体的参数如下。12$ git reset --hard HEAD^ //^数量表示当前版本的前几个，数较大时HEAD~50$ git reset --hard 版本号 //是的每次修改都有一个对应的版本号可以直接用上面是版本之间的跳转，当你只提交到暂存区时想回退怎么办呢，又或是刚在工作区修改完想回退怎么办呢?123$ git checkout -- 文档名 //工作区修改撤销$ git reset HEAD 文档名 //以提交到暂存区的修改撤销，此句撤销到工作区 //还需接着运行上一句一并撤销工作区的修改文件的删除通常使用$git rm 文件名只有执行了这句才把文件从git版本库里真正删除，直接在目录下（工作区）删除后，版本库里还存在被删除文件的信息。分支运用bug修复时，用$ git stash把当前进度存储，再运用分支的便利性创建bug分支进行更改。修改完毕后再用$ git stash pop恢复刚才的进度。把本地写好的分支推送到远程时用命令$ git push 远程库名 本地要推的分支名常规clone远程库到本地后，只有master分支，想要在其他分支工作需要$ git checkout -b 目标分支名本地 origin/目标分支名远程以此来和远程库分支对应。创建以后还要$ git branch --set-upstream 本地分支名 origin/远程分支名$ git pull从远程抓取分支提交信息，用于本地分支推送失败时排查冲突。]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown简易教程]]></title>
    <url>%2F2015%2F01%2F29%2Fmarkdown%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[教程说明本教程简略的列出了markdown最常用的基础用法，便于快速上手。关于进一步的用法技巧将在第二部分基础语法后逐步完善积累。基础语法标题：“ # ”可用于标题标记，符号的个数依次对应标题的阶数（例如基础语法前有一个符号“ # ”，以下的小标题依次加一个“#”）。当进入下一个大标题例如“2.”时，“#”再从一个依次开始。强调：对需强调的词语两端各加一个 “ * ” 。列表：对于无序列表只需在各标题前加“ + ”或者“ - ”，还有“ * ”也可以，不过记一个就行了。具体事例如下。我前面有个 “ - ”。我前面有个 “ + ”。我前面有个 “ * ”。链接：分为行内和参考两种形式。行内形式链接放圆括弧，文字放方括弧,例如微风拂来。参考形式是在段落链接较多时，把方括弧后的圆括弧还为响应的数字，如微风拂来紧接着下面标出每个数字对应的域名。1[1]: breeze.ink代码：一般在编辑器里面有直接插入代码块的功能，这里推荐Madoko在线编辑器或者Haroopad线下编辑器，二者都可以在markdown编写的过程中即时可视化页面，另外有很多强大的功能Madoko。图片：图片的写法参考链接的写法写法如下。另外在madoko编辑器中有直接插入图片的功能，类似的还有许多这样的快捷功能键。1![图片名称](../img.jpg "Title")大规模的图片存储要用到外链图床服务，推荐七牛云。具体操作步骤参考使用七牛存储图片进阶语法##实现文本指定位置跳转在写文章时，有时我们需要赋予一个词链接效果，可以跳转到文章指定位置，多数情况跳转到某个标题处。注意name 参数可以自定义，&lt;/a&gt;记得要加，不然后文都成了链接。123[赋予跳转效果的词语](#name)……&lt;a id='name'&gt;标题&lt;/a&gt;]]></content>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
</search>
